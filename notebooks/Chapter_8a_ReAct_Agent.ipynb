{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8a — ReAct Agent (Reason + Act)\n",
    "\n",
    "**Pattern:** A single agent that interleaves reasoning and tool use — it thinks about what to do, acts, observes the result, and re-plans after every step.\n",
    "\n",
    "```\n",
    "Goal → [Think → Act → Observe → Think → Act → Observe → ...] → Final Answer\n",
    "```\n",
    "\n",
    "### How This Builds on Earlier Chapters\n",
    "\n",
    "| Chapter | Pattern | What we learned | How ReAct extends it |\n",
    "|---------|---------|-----------------|---------------------|\n",
    "| Ch 5 | Tool Use | Agents call tools to get live data | ReAct chains **multiple tool calls** with reasoning between each |\n",
    "| Ch 7 | Sequential | Agents run in a fixed order | ReAct has **no fixed order** — the agent decides what's next after each observation |\n",
    "| Ch 6 | Reflection | A reviewer checks output against rules | ReAct reflects **inline** — every Thought step is a mini-reflection |\n",
    "\n",
    "### When to Use ReAct vs Other Patterns\n",
    "\n",
    "- **Exploratory tasks** where you don't know how many steps you'll need\n",
    "- **Search-heavy tasks** where each result changes what to do next\n",
    "- **Short-horizon tasks** (2–5 steps) where a full plan is overkill\n",
    "- When **all reasoning should be visible** in one agent's trace\n",
    "\n",
    "### The Scenario\n",
    "\n",
    "You're a **tech lead** evaluating whether to adopt Rust for a performance-critical microservice. You need to research Rust's memory model, compare it to your team's current Go stack, write up a recommendation, and verify the key claims. The research is *exploratory* — what you learn from one search changes what you search for next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API Key set: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "assert os.environ.get(\"GOOGLE_API_KEY\"), \"Set GOOGLE_API_KEY first\"\n",
    "print(\"Google API Key set:\", bool(os.environ.get(\"GOOGLE_API_KEY\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.tools import FunctionTool\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tools — Mock Knowledge Base + Notes System\n",
    "\n",
    "We use **deterministic mocks** (same lesson from Ch 6: mock internals for reproducibility). The mock search returns realistic results that create an interesting research path — each result raises new questions.\n",
    "\n",
    "| Tool | Purpose | Why the agent needs it |\n",
    "|------|---------|------------------------|\n",
    "| `web_search` | Look up technical topics | Each search result informs what to search next |\n",
    "| `write_note` | Save a research finding | Builds up a knowledge base across steps |\n",
    "| `list_notes` | See what's been researched | Helps the agent avoid redundant searches |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools defined: web_search, write_note, list_notes\n"
     ]
    }
   ],
   "source": [
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for technical information. Returns a summary of findings.\"\"\"\n",
    "    # Deterministic mock — realistic results that create an interesting research path\n",
    "    knowledge_base = {\n",
    "        \"rust ownership\": (\n",
    "            \"Rust's ownership system guarantees memory safety without a garbage collector. \"\n",
    "            \"Every value has exactly one owner. When the owner goes out of scope, the value is dropped. \"\n",
    "            \"Borrowing rules: you can have either one mutable reference OR multiple immutable references. \"\n",
    "            \"This eliminates data races at compile time. Zero-cost abstractions mean no runtime overhead.\"\n",
    "        ),\n",
    "        \"rust vs go performance\": (\n",
    "            \"Benchmarks show Rust is 2-5x faster than Go for CPU-bound tasks due to zero-cost abstractions \"\n",
    "            \"and no GC pauses. Go excels in developer productivity and has faster compile times. \"\n",
    "            \"Rust's memory model eliminates GC pause spikes — critical for p99 latency requirements. \"\n",
    "            \"Go's goroutines make concurrent I/O simpler but Rust's async/await with tokio matches throughput.\"\n",
    "        ),\n",
    "        \"rust learning curve\": (\n",
    "            \"Rust has a steep learning curve — most teams report 3-6 months to productivity. \"\n",
    "            \"The borrow checker causes friction for developers from GC languages. \"\n",
    "            \"However, Rust's compiler errors are highly informative and guide fixes. \"\n",
    "            \"Teams report fewer production bugs after the learning period — the compiler catches issues upfront.\"\n",
    "        ),\n",
    "        \"rust adoption enterprise\": (\n",
    "            \"Major adopters: AWS (Firecracker), Microsoft (Windows kernel), Cloudflare (edge workers), \"\n",
    "            \"Discord (switched from Go to Rust for latency-sensitive services). \"\n",
    "            \"Discord's case study: Go's GC pauses caused latency spikes every 2 minutes. \"\n",
    "            \"After rewriting in Rust, p99 latency dropped from 300ms to 10ms. Memory usage halved.\"\n",
    "        ),\n",
    "        \"go microservice\": (\n",
    "            \"Go strengths for microservices: fast compilation, simple concurrency model (goroutines), \"\n",
    "            \"small binary sizes, excellent standard library for HTTP/networking. \"\n",
    "            \"Weakness: GC pauses (typically 1-3ms but can spike to 10ms+ under heap pressure). \"\n",
    "            \"Go 1.22 improved GC but latency-sensitive services still need careful tuning.\"\n",
    "        ),\n",
    "    }\n",
    "    for key, result in knowledge_base.items():\n",
    "        if key in query.lower():\n",
    "            return result\n",
    "    return f\"Search results for '{query}': Several relevant articles found covering this topic.\"\n",
    "\n",
    "\n",
    "def write_note(title: str, content: str) -> str:\n",
    "    \"\"\"Save a research note to the local filesystem.\"\"\"\n",
    "    path = f\"/tmp/react_note_{title.replace(' ', '_').lower()}.txt\"\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(f\"# {title}\\n\\n{content}\")\n",
    "    return f\"Note saved: {path} ({len(content)} chars)\"\n",
    "\n",
    "\n",
    "def list_notes() -> str:\n",
    "    \"\"\"List all saved research notes.\"\"\"\n",
    "    files = glob.glob(\"/tmp/react_note_*.txt\")\n",
    "    if not files:\n",
    "        return \"No notes saved yet.\"\n",
    "    result = []\n",
    "    for f in sorted(files):\n",
    "        with open(f) as fh:\n",
    "            first_line = fh.readline().strip()\n",
    "        result.append(f\"  {f} — {first_line}\")\n",
    "    return \"Saved notes:\\n\" + \"\\n\".join(result)\n",
    "\n",
    "\n",
    "print(\"Tools defined: web_search, write_note, list_notes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The ReAct Agent\n",
    "\n",
    "A **single `LlmAgent`** with an instruction that encourages step-by-step reasoning **between** tool calls.\n",
    "\n",
    "The key insight: we do **not** ask the model to write out \"Action:\" or \"Observation:\" labels — that causes Gemini to role-play tool use in plain text instead of actually calling tools through ADK. Instead, we tell it to:\n",
    "\n",
    "1. **Think before each tool call** — explain what you're doing and why\n",
    "2. **Use the actual tools** — ADK handles the function calling protocol\n",
    "3. **Reflect after each result** — update your approach based on what you learned\n",
    "4. **Call tools multiple times** — don't try to answer everything in one shot\n",
    "\n",
    "The `FunctionTool` wrapper (from `google.adk.tools`) converts plain Python functions into tools that ADK can expose to Gemini. ADK reads the function signature + docstring to generate the JSON schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReAct agent ready with tools: ['web_search', 'write_note', 'list_notes']\n"
     ]
    }
   ],
   "source": [
    "REACT_INSTRUCTION = \"\"\"\n",
    "You are a research agent that reasons step-by-step and uses tools to gather information.\n",
    "\n",
    "Your process for EVERY task:\n",
    "1. Think about what information you need and explain your reasoning briefly.\n",
    "2. Call the appropriate tool (web_search, write_note, or list_notes).\n",
    "3. After getting the result, reflect on what you learned and decide what to do next.\n",
    "4. Repeat steps 1-3 as many times as needed — do NOT try to answer in one step.\n",
    "5. When you have enough information, save your key findings using write_note.\n",
    "6. Finally, provide your complete answer.\n",
    "\n",
    "IMPORTANT RULES:\n",
    "- You MUST actually call the tools — do not simulate or role-play tool usage in text.\n",
    "- Call web_search multiple times for different aspects of the research.\n",
    "- After each search result, think about what ELSE you need to learn before proceeding.\n",
    "- Save important findings as notes before giving your final answer.\n",
    "- Be thorough — research at least 3 different aspects before concluding.\n",
    "\"\"\"\n",
    "\n",
    "react_agent = LlmAgent(\n",
    "    name=\"ReActResearcher\",\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    instruction=REACT_INSTRUCTION,\n",
    "    description=\"Single agent that reasons and acts in an interleaved loop using real tool calls.\",\n",
    "    tools=[\n",
    "        FunctionTool(web_search),\n",
    "        FunctionTool(write_note),\n",
    "        FunctionTool(list_notes),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"ReAct agent ready with tools:\", [t.name for t in react_agent.tools])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Runner — Streaming the ReAct Loop\n",
    "\n",
    "We stream events so you can **watch the reasoning happen** — every Thought, every tool call, every observation is visible. This is the key advantage of ReAct: the entire reasoning trace is in one agent, making it easy to debug.\n",
    "\n",
    "Compare this to Ch 7's multi-agent patterns where reasoning is split across agents. ReAct trades modularity for transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runner function defined: run_react()\n"
     ]
    }
   ],
   "source": [
    "async def run_react(goal: str, user_id: str = \"user_001\"):\n",
    "    \"\"\"Run the ReAct agent on a goal and stream the reasoning trace.\"\"\"\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"GOAL: {goal}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    session_service = InMemorySessionService()\n",
    "    runner = Runner(\n",
    "        agent=react_agent,\n",
    "        app_name=\"react_research\",\n",
    "        session_service=session_service,\n",
    "    )\n",
    "\n",
    "    session = await session_service.create_session(\n",
    "        app_name=\"react_research\", user_id=user_id,\n",
    "    )\n",
    "\n",
    "    content = types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part.from_text(text=goal)],\n",
    "    )\n",
    "\n",
    "    step_count = 0\n",
    "    async for event in runner.run_async(\n",
    "        user_id=user_id, session_id=session.id, new_message=content,\n",
    "    ):\n",
    "        if event.content:\n",
    "            for part in event.content.parts:\n",
    "                # Tool calls — the \"Act\" in ReAct\n",
    "                if hasattr(part, \"function_call\") and part.function_call:\n",
    "                    fc = part.function_call\n",
    "                    step_count += 1\n",
    "                    print(f\"\\n  [{step_count}] TOOL CALL: {fc.name}({dict(fc.args)})\")\n",
    "\n",
    "                # Tool results — the \"Observe\" in ReAct\n",
    "                elif hasattr(part, \"function_response\") and part.function_response:\n",
    "                    fr = part.function_response\n",
    "                    result_str = str(fr.response)[:200]\n",
    "                    print(f\"      RESULT: {result_str}{'...' if len(str(fr.response))>200 else ''}\")\n",
    "\n",
    "                # Text — the \"Think\" in ReAct\n",
    "                elif part.text and part.text.strip():\n",
    "                    print(f\"\\n{part.text}\")\n",
    "\n",
    "        if event.is_final_response():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"DONE — {step_count} tool calls\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "print(\"Runner function defined: run_react()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run It — Exploratory Research Task\n",
    "\n",
    "Watch the agent's reasoning unfold. Notice how each search result **changes what it searches for next** — this is the core strength of ReAct over fixed sequential pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GOAL: Research Rust's ownership model and compare it to Go for building latency-sensitive microservices. Look into real-world adoption cases. Save your key findings as notes, then give me a recommendation on whether our team should adopt Rust for our new payment gateway service.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Okay, I need to research Rust's ownership model, compare it to Go for building latency-sensitive microservices, and look into real-world adoption cases. Then, I will provide a recommendation on adopting Rust for a new payment gateway service.\n",
      "\n",
      "First, I'll start by researching Rust's ownership model.\n",
      "\n",
      "\n",
      "  [1] TOOL CALL: web_search({'query': 'Rust ownership model'})\n",
      "      RESULT: {'result': \"Rust's ownership system guarantees memory safety without a garbage collector. Every value has exactly one owner. When the owner goes out of scope, the value is dropped. Borrowing rules: yo...\n",
      "\n",
      "The ownership model seems to be a core feature ensuring memory safety and preventing data races at compile time. Now, I'll research Go's memory management and concurrency model and compare that to Rust.\n",
      "\n",
      "\n",
      "  [2] TOOL CALL: web_search({'query': 'Go memory management and concurrency model vs Rust'})\n",
      "      RESULT: {'result': \"Search results for 'Go memory management and concurrency model vs Rust': Several relevant articles found covering this topic.\"}\n",
      "\n",
      "Okay, that's not very specific. Let me refine the search query.\n",
      "\n",
      "\n",
      "  [3] TOOL CALL: web_search({'query': 'Go garbage collection vs Rust ownership'})\n",
      "      RESULT: {'result': \"Rust's ownership system guarantees memory safety without a garbage collector. Every value has exactly one owner. When the owner goes out of scope, the value is dropped. Borrowing rules: yo...\n",
      "\n",
      "Still pretty generic. I need to be more specific.\n",
      "\n",
      "\n",
      "  [4] TOOL CALL: web_search({'query': 'Go garbage collection latency vs Rust'})\n",
      "      RESULT: {'result': \"Search results for 'Go garbage collection latency vs Rust': Several relevant articles found covering this topic.\"}\n",
      "\n",
      "Still not great. Let's try another angle:\n",
      "\n",
      "\n",
      "  [5] TOOL CALL: web_search({'query': 'Rust vs Go latency-sensitive microservices'})\n",
      "      RESULT: {'result': \"Search results for 'Rust vs Go latency-sensitive microservices': Several relevant articles found covering this topic.\"}\n",
      "\n",
      "Okay, still vague. I will try searching for specific articles that compare the performance.\n",
      "\n",
      "\n",
      "  [6] TOOL CALL: web_search({'query': 'performance comparison Rust vs Go microservices'})\n",
      "      RESULT: {'result': 'Go strengths for microservices: fast compilation, simple concurrency model (goroutines), small binary sizes, excellent standard library for HTTP/networking. Weakness: GC pauses (typically ...\n",
      "\n",
      "  [7] TOOL CALL: web_search({'query': 'Rust microservices performance benefits'})\n",
      "      RESULT: {'result': \"Search results for 'Rust microservices performance benefits': Several relevant articles found covering this topic.\"}\n",
      "\n",
      "Let me try a more specific query:\n",
      "\n",
      "\n",
      "  [8] TOOL CALL: web_search({'query': 'Rust zero-cost abstractions microservices'})\n",
      "      RESULT: {'result': \"Search results for 'Rust zero-cost abstractions microservices': Several relevant articles found covering this topic.\"}\n",
      "\n",
      "Okay, the search results are consistently pointing towards relevant articles. I need to summarize the key points.\n",
      "\n",
      "Go:\n",
      "*   Fast compilation\n",
      "*   Simple concurrency with goroutines\n",
      "*   Small binary sizes\n",
      "*   Good standard library for networking\n",
      "*   Garbage collection pauses (1-3ms, can spike)\n",
      "\n",
      "Rust:\n",
      "*   No garbage collection\n",
      "*   Zero-cost abstractions\n",
      "*   Memory safety\n",
      "*   Compile-time error checking\n",
      "\n",
      "Now, let's look for real-world adoption cases in payment gateways.\n",
      "\n",
      "\n",
      "  [9] TOOL CALL: web_search({'query': 'Rust adoption payment gateways'})\n",
      "      RESULT: {'result': \"Search results for 'Rust adoption payment gateways': Several relevant articles found covering this topic.\"}\n",
      "\n",
      "Again, not very specific. Let's try a different search.\n",
      "\n",
      "\n",
      "  [10] TOOL CALL: web_search({'query': 'payment gateway written in Rust'})\n",
      "      RESULT: {'result': \"Search results for 'payment gateway written in Rust': Several relevant articles found covering this topic.\"}\n",
      "\n",
      "Okay, I'm not getting specific examples of payment gateways written in Rust, but the generic results suggest there are examples out there. I will now save my notes.\n",
      "\n",
      "\n",
      "  [11] TOOL CALL: write_note({'content': 'Go:\\n* Fast compilation\\n* Simple concurrency with goroutines\\n* Small binary sizes\\n* Good standard library for networking\\n* Garbage collection pauses (1-3ms, can spike)\\n\\nRust:\\n* No garbage collection\\n* Zero-cost abstractions\\n* Memory safety\\n* Compile-time error checking', 'title': 'Rust vs Go for Latency-Sensitive Microservices'})\n",
      "      RESULT: {'result': 'Note saved: /tmp/react_note_rust_vs_go_for_latency-sensitive_microservices.txt (268 chars)'}\n",
      "\n",
      "Now, I need to save my other finding related to GC pauses.\n",
      "\n",
      "\n",
      "  [12] TOOL CALL: write_note({'title': 'Go GC Pauses', 'content': 'Go GC pauses are typically 1-3ms but can spike to 10ms+ under heap pressure. Go 1.22 improved GC but latency-sensitive services still need careful tuning.'})\n",
      "      RESULT: {'result': 'Note saved: /tmp/react_note_go_gc_pauses.txt (154 chars)'}\n",
      "\n",
      "I've gathered sufficient information. Now I will provide a recommendation.\n",
      "\n",
      "Recommendation:\n",
      "\n",
      "For a new payment gateway service, **Rust is the more suitable choice**, despite the potentially steeper learning curve. The critical factor is latency sensitivity. Go's garbage collection, while generally efficient, introduces pauses that can unpredictably impact transaction processing times. In a payment gateway, even millisecond delays can be unacceptable. Rust's ownership model eliminates the need for a garbage collector, ensuring consistently low and predictable latency. While Go offers faster compilation and a simpler concurrency model, the performance benefits of Rust's zero-cost abstractions and memory safety without GC outweigh these advantages for this particular use case. Although finding specific adoption examples in payment gateways has been challenging, the general consensus from performance comparisons indicates Rust is better suited for latency-critical applications. The increased development complexity with Rust can be mitigated through careful planning, code reviews, and leveraging existing Rust libraries.\n",
      "\n",
      "\n",
      "============================================================\n",
      "DONE — 12 tool calls\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "await run_react(\n",
    "    \"Research Rust's ownership model and compare it to Go for building \"\n",
    "    \"latency-sensitive microservices. Look into real-world adoption cases. \"\n",
    "    \"Save your key findings as notes, then give me a recommendation \"\n",
    "    \"on whether our team should adopt Rust for our new payment gateway service.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Deep Dive: What Actually Happened Inside the Loop\n\nTo understand the ReAct pattern, you need to see **three actors** working together: your Python code (the tools), ADK (the middleman), and Gemini (the brain). Your `run_react()` function is just a window — it prints events as they stream by but doesn't drive the reasoning.\n\n### The Three Actors\n\n```\nYour Code                          ADK (invisible)                    Gemini (the brain)\n─────────                          ───────────────                    ──────────────────\n                                   sends message + tools ──────────►  \n                                                                      thinks: \"I need to search\"\n                                   ◄────────── function_call: web_search(\"Rust ownership model\")\n                                   runs YOUR Python function\n                                   sends result back ──────────────►\n                                                                      thinks: \"good, now I need Go info\"\n                                   ◄────────── function_call: web_search(\"Go memory management...\")\n                                   runs YOUR Python function\n                                   sends result back ──────────────►\n                                                                      thinks: \"that was vague, retry\"\n                                   ◄────────── function_call: web_search(\"Go garbage collection...\")\n                                   ...repeats until satisfied...\n                                   ◄────────── text only (final answer)\nyou print each event\nas it streams by\n```\n\n**ADK calls Gemini multiple times** inside a single `runner.run_async()` call. Each time Gemini returns a `function_call`, ADK runs your Python function and sends the result back. This continues until Gemini responds with text only (no more tool calls).\n\n### Tracing Each Step Against the Knowledge Base\n\nOur mock `web_search` has exactly **5 keys** that return real data. Everything else hits a generic fallback:\n\n```python\nknowledge_base = {\n    \"rust ownership\"           →  detailed Rust ownership explanation\n    \"rust vs go performance\"   →  benchmarks, p99 latency comparison\n    \"rust learning curve\"      →  3-6 months, borrow checker friction\n    \"rust adoption enterprise\" →  AWS, Discord, Cloudflare case studies\n    \"go microservice\"          →  Go strengths, GC pause weakness\n}\n# Anything else returns: \"Search results for '...': Several relevant articles found.\"\n```\n\nThe matching logic is `if key in query.lower()` — the key must appear as a substring in the query.\n\n---\n\n**Step [1]** — Query: `\"Rust ownership model\"`\n\nDoes `\"rust ownership\"` appear in this query? **Yes.** Returns detailed Rust ownership explanation. Gemini gets rich data and proceeds.\n\n**Step [2]** — Query: `\"Go memory management and concurrency model vs Rust\"`\n\nCheck each key: `\"rust ownership\"` in query? No. `\"rust vs go performance\"`? No. `\"go microservice\"`? No. **None match** → generic fallback.\n\nGemini compares this vague result to Step 1's rich response and reasons: *\"That's not very specific. Let me refine.\"*\n\n**Step [3]** — Query: `\"Go garbage collection vs Rust ownership\"`\n\n`\"rust ownership\"` in `\"go garbage collection vs rust ownership\"`? **Yes!** But this returns the *Rust* explanation again — not the Go info Gemini wanted. Gemini says: *\"Still pretty generic.\"*\n\n**Step [4]** — Query: `\"Go garbage collection latency vs Rust\"`\n\nNo key matches → **generic fallback**. Gemini: *\"Still not great.\"*\n\n**Step [5]** — Query: `\"Rust vs Go latency-sensitive microservices\"`\n\nClose! But the key is `\"rust vs go performance\"` and the query says `\"rust vs go latency-sensitive microservices\"`. No exact substring match → **generic fallback**.\n\n**Step [6]** — Query: `\"performance comparison Rust vs Go microservices\"`\n\n`\"go microservice\"` in `\"performance comparison rust vs go microservices\"`? **Yes!** (The query contains `\"go microservices\"` which includes the substring `\"go microservice\"`.) Finally returns real Go data: GC pauses, goroutines, compilation speed.\n\n**Steps [7-8]** — More searches, generic fallbacks. Gemini decides it has enough from steps [1], [3], and [6].\n\n**Steps [9-10]** — Searches for payment gateway adoption cases. No keys match → fallback. Gemini acknowledges the gap and moves on.\n\n**Steps [11-12]** — Gemini saves its findings as notes using `write_note`, then delivers the final recommendation.\n\n---\n\n### The Key Insight\n\n**Your code never told Gemini to retry or refine its queries.** The `run_react()` function just prints events. The retry behavior is Gemini's own reasoning — it saw a vague result, judged it insufficient compared to earlier rich results, and autonomously decided to try a different query. That's the core of the ReAct pattern: the LLM self-corrects after every observation.\n\nThis is also why ReAct made **12 tool calls** instead of the 5 you might expect (one per knowledge base entry). The mock's limited key matching forced Gemini to be creative with its queries — and you got to watch it adapt in real time.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Try a Different Goal\n",
    "\n",
    "The same agent works on any exploratory task. Try changing the goal to see how the reasoning path changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run with your own goal:\n",
    "# await run_react(\"Research the pros and cons of on-device AI inference vs cloud inference. Save a comparison note.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "### ReAct vs Patterns from Earlier Chapters\n",
    "\n",
    "| Dimension | Ch 5 Tool Use | Ch 7 Sequential | Ch 6 Reflection | **Ch 8a ReAct** |\n",
    "|-----------|---------------|-----------------|-----------------|----------------|\n",
    "| Tool calls | One | Fixed chain | Separate reviewer | **Many, dynamic** |\n",
    "| Planning | None | Pre-defined | Post-hoc review | **Inline, after every step** |\n",
    "| Execution order | Single step | Fixed A→B→C | Generate→Review loop | **Emergent from reasoning** |\n",
    "| Debugging | Easy | Trace across agents | Two-agent trace | **Single agent trace** |\n",
    "| Best for | Simple lookups | Assembly-line tasks | Compliance checking | **Exploratory research** |\n",
    "\n",
    "### Critical Design Lesson: Don't Ask the Model to Role-Play Tool Use\n",
    "\n",
    "The original ReAct paper uses a `Thought: / Action: / Observation:` text format. But with ADK, this is a **trap** — if you put those labels in the system prompt, Gemini will *write out* the tool calls as text instead of *actually calling* the tools via ADK's function calling protocol.\n",
    "\n",
    "The fix: tell the agent to **use the tools directly** and reason between calls. ADK handles the function calling mechanics. You get the same Think→Act→Observe loop, but through real tool invocations that show up as `function_call` / `function_response` events.\n",
    "\n",
    "### Design Decisions\n",
    "\n",
    "- **Simple, direct instruction** — No \"Thought/Action/Observation\" labels. Just tell the agent to reason, use tools, and reflect after each result. ADK's function calling does the rest.\n",
    "- **`FunctionTool` wrapper** — Converts plain Python functions into ADK tools. ADK reads the function signature + docstring to generate the JSON schema Gemini needs.\n",
    "- **Deterministic mocks** — Same principle as Ch 6: mock data makes the research path reproducible and testable.\n",
    "- **No plan commitment** — Unlike Ch 8b (Plan-and-Execute), ReAct doesn't commit to a plan upfront. Each step is decided *after* seeing the previous result.\n",
    "\n",
    "### When NOT to Use ReAct\n",
    "\n",
    "- **Structured multi-step tasks** (6+ steps) — Use Plan-and-Execute (Ch 8b) instead\n",
    "- **Tasks needing an auditable plan** — ReAct's plan is implicit in the trace, not a standalone artifact\n",
    "- **Parallelizable work** — ReAct is inherently serial. Use ParallelAgent (Ch 3) for concurrent execution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ADK)",
   "language": "python",
   "name": "adk_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}